---
layout:     post
title:      深度学习踩坑
subtitle:   改进
date:       2018-08-21
author:     Zach
header-img: img/post-bg-field.jpg
catalog: true
tags:
    - TensorFlow
    - DeepLearning
---

- 如果出现某batch样本极度不均衡，如全为负样本，经由这种batch训练后会剧烈改变模型的参数
![示例](https://raw.githubusercontent.com/Zachary4biz/Tahiti_any_data/master/pics/result_of_all_negtive_in_batch.png "Optional title"){:height=50% width=50%}
- dropout在train中使用，在训练集自己测定auc以及验证集测定auc（或logloss）时，需要取消dropout参数。
    + tf实现dropout的机制就是让一个神经元以固定的概率失活
    + ```"dropout_deep": [0.8, 0.5, 0.5, 0.5]``` 可以看出有三个隐层，对输入层的神经元是0.8的概率活跃，三个隐层的神经元都是0.5的概率活跃
    + 这里是可能出现极端情况，百分之八九十的神经元都失活了。

- 先增加网络复杂度，让网络产生过拟合，再调整dropout、batchNorm、正则 等
    + >“Ng课里也说越多参数的全链接层要drop out的概率就大一点”

- 观察Tensorboard如果发现某些参数（如DeepFM中w0、feature_embedding）更新很小甚至没有更新时，导致梯度消失可能的情况如下
    + 内参初始化问题，例如对feature_embedding采用[-0.01,0.01]迭代效果会比[0,0.1]更好，而如果使用[0,1]初始化会导致严重的梯度消失
    + 模型结构过于复杂





