---
layout:     post
title:      单机多GPU——DeepFM
subtitle:   改进
date:       2018-08-21
author:     Zach
header-img: img/post-bg-kuaidi.jpg
catalog: true
tags:
    - DeepFM
    - 单机多GPU
    - 已完成
---
## 概要
- ```tower_losses```、```tower_grads``` 为什么变量名命名是```tower_xxx```？
    + [TensorFlow的官方guide](https://www.tensorflow.org/guide/using_gpu) 中提到了一种叫```multi-tower```的方式来构建模型，把每个```tower```指派给不同的GPU
    + 搜索```gpu tower```会得到如下一堆服务器的图片<br/>
    ![gpu tower](https://raw.githubusercontent.com/Zachary4biz/Tahiti_any_data/master/pics/search_tower_gpu_img.jpg "Optional title"){:height="60%" width="60%"}
    + 所以应该就是利用了*tower*包含的“叠加、层叠”的意思

## 多GPU模型实现思路
- 搭建DeepFM模型计算图 [代码附录](#DeepFM-graph)
- 3个GPU在各自的```variable_scope```中创建各自的计算图 [代码附录](#multi_gpu_graph)
    * 这里导致后续传入batch数据的时候要手动分成三份，提供给3个GPU各自的```tf.placeholder```
    * 注意这里```optimizer```只调用```compute_gradient```做求解梯度的部分，不直接进行```minimize```，后面用梯度的均值进行梯度下降
- 3个GPU各自的梯度再求均值 [代码附录](#average_gradient)
- 使用```optimizer.apply_gradients```，用平均梯度进行更新
 ![multi-gpu](https://raw.githubusercontent.com/Zachary4biz/Tahiti_any_data/master/pics/multi_gpu_outline.jpg){:height="80%" width="90%"}

## DeepFM 介绍
DeepFM的最常见的模型结构图如下<br/>
<div align=center>![DeepFM 图](https://raw.githubusercontent.com/Zachary4biz/Tahiti_any_data/master/pics/DeepFM.png "Optional title"){:height="70%" width="70%"}</div>

- FM的介绍：
    + [FM比较详细的公式说明](http://www.360doc.com/content/18/0118/13/47852988_723049882.shtml)
    + [简介&与其他模型的对比](https://www.cnblogs.com/hxsyl/p/5255427.html)
    + FM主要思想就是做分解，把样本的每个feature_field都用一个n维的隐向量来表示
        * 通常一个feature_field用 one-hot 编码后的向量有几十上百维，甚至更多
        * 用来表示这个feature_field的隐向量通常取 6维、8维、10维 <br/>
        ![embeddings_example](https://raw.githubusercontent.com/Zachary4biz/Tahiti_any_data/master/pics/embeddings_example.jpg "Optional title"){:height="60%" width="60%"}
        * 隐向量(embeddings)通常随机初始化为 0~0.01 之间
        * 所有特征都做过embeddings后，[ 样本 x 特征 ] 矩阵如下：<br/>
        ![embeddings_matrix](https://raw.githubusercontent.com/Zachary4biz/Tahiti_any_data/master/pics/embeddings_matrix.jpg "Optional title"){:height="90%" width="90%"}
- Deep介绍：
    + 使用embeddings向量作为输入
- Deep&FM ：
    + FM的输出和Deep的输出concat到一起，然后做一个全连接，输出一个predict_label



## 本次示例代码对DeepFM应用如下：
- 使用数据是Criteo在2014年发起的Kaggle比赛用的广告数据 [数据集附录](#criteo_data)
- 连续特征、离散特征作为FM部分的输入，实现"二阶交叉"
- embedding向量作为Deep部分的输入, 312维输入(39个feature_field x 8维embedding)
- [GitHub代码](https://github.com/Zachary4biz/algrithm/blob/master/python/7-Tensorflow/tensorflow-DeepFM-master/DeepFM_use_generator_gpu.py)

## 部分变量的结构
- weights字典如下:
    + feature_embeddings: 11w特征每个都有一个初始化的8维隐向量
        * ```<tf.Variable 'feature_embeddings_2:0' shape=(117581, 8) dtype=float32_ref>```
    + feature_bias: 11w特征每个都有一个初始化的1维偏置（1个数）
        * ```<tf.Variable 'feature_bias_2:0' shape=(117581, 1) dtype=float32_ref>```
    + layer_0: 隐层的权重
        * ```<tf.Variable 'w_layer_0_1:0' shape=(312, 32) dtype=float32_ref>```
    + bias_0: 隐层的偏置
        * ```<tf.Variable 'b_layer_0_1:0' shape=(1, 32) dtype=float32_ref>```
    + layer_1: 隐层的权重
        * ```<tf.Variable 'w_layer_1_1:0' shape=(32, 32) dtype=float32_ref>```
    + bias_1: 隐层的偏置
        * ```<tf.Variable 'b_layer_1_1:0' shape=(1, 32) dtype=float32_ref>```
    + concat_projection: concat输出层的权重 
        * ```<tf.Variable 'concat_projection_1:0' shape=(79, 1) dtype=float32_ref>```
    + concat_bias: concat输出层的偏置
        * ```<tf.Variable 'concat_bias_1:0' shape=() dtype=float32_ref>```

- models [详细结构](#inside_struct_models)
    + ```models```<br/>
    ![models 示例](https://raw.githubusercontent.com/Zachary4biz/Tahiti_any_data/master/pics/multi_gpu_models.png "Optional title"){:height="60%" width="60%"}
    + ```zip(*models)``` <br/>
    ![zip models ](https://raw.githubusercontent.com/Zachary4biz/Tahiti_any_data/master/pics/multi_gpu_zip_models.png){:height="60%" width="60%"}

- ```tower_grads``` 是一个tuple，每个元都表示是一个gpu上的梯度```grads```
    + 一些帮助理解的数据：
        * 本例使用3个GPU，则这里的```len(tower_grads)```为3
        * 每个grad有8层。输入层embedding、2个隐层+输出层concat，每层都有w和b两部分，一共是8个，```len(tower_grads[0])``` 为8
    + ```tower_grads[0]``` gpu0上的数据计算梯度，长度为8的list：
        * 结构如下（和上文的 ```grads``` 结构一样，就是GPU:0上的```grads```）：
```python3
"""
    [(tensor, varaible), (tensor, varaible), (tensor, varaible) ...]
"""
(
    # 输入层 embeddings
    <tensorflow.python.framework.ops.IndexedSlices object at 0x7fd35777edd8>, <tf.Variable 'feature_embeddings:0' shape=(117581, 8) dtype=float32_ref>)
    (<tensorflow.python.framework.ops.IndexedSlices object at 0x7fd35784ff60>, <tf.Variable 'feature_bias:0' shape=(117581, 1) dtype=float32_ref>)
    # 隐层1
    (<tf.Tensor 'tower_0_4/gpu_variables/gradients/tower_0_4/gpu_variables/MatMul_grad/tuple/control_dependency_1:0' shape=(312, 32) dtype=float32>, <tf.Variable 'w_layer_0:0' shape=(312, 32) dtype=float32_ref>)
    (<tf.Tensor 'tower_0_4/gpu_variables/gradients/tower_0_4/gpu_variables/Add_grad/tuple/control_dependency_1:0' shape=(1, 32) dtype=float32>, <tf.Variable 'b_layer_0:0' shape=(1, 32) dtype=float32_ref>)
    # 隐层2
    (<tf.Tensor 'tower_0_4/gpu_variables/gradients/tower_0_4/gpu_variables/MatMul_1_grad/tuple/control_dependency_1:0' shape=(32, 32) dtype=float32>, <tf.Variable 'w_layer_1:0' shape=(32, 32) dtype=float32_ref>)
    (<tf.Tensor 'tower_0_4/gpu_variables/gradients/tower_0_4/gpu_variables/Add_1_grad/tuple/control_dependency_1:0' shape=(1, 32) dtype=float32>, <tf.Variable 'b_layer_1:0' shape=(1, 32) dtype=float32_ref>)
    # 输出层 concat
    (<tf.Tensor 'tower_0_4/gpu_variables/gradients/tower_0_4/gpu_variables/MatMul_2_grad/tuple/control_dependency_1:0' shape=(79, 1) dtype=float32>, <tf.Variable 'concat_projection:0' shape=(79, 1) dtype=float32_ref>)
    (<tf.Tensor 'tower_0_4/gpu_variables/gradients/tower_0_4/gpu_variables/Add_2_grad/tuple/control_dependency_1:0' shape=() dtype=float32>, <tf.Variable 'concat_bias:0' shape=() dtype=float32_ref>
)
```
    * [tower_grads触发计算后的示例](#tower_grads)

## 报错及解决
- 报错：```None values not supported.```
    + 发现 ```compute_gradients``` 会有很多个为 None 的，导致触发了```None values not supported.```报错。
    + 目前的解决方法是不给Variable加名字，可以避免这个错误。
    + 更新：原因推测是构建模型的时候，使用for循环对同一个变量重复使用，如下代码。具体内部原因暂时没抽出时间去深究，应该是复用变量时，当前的```variable_scope```又在变(多GPU
    )、还使用了```reuse```导致的，一般单机单GPU，以及之前测试分布式多CPU，没有用到 ```varaible_scope``` 和 ```reuse``` 都没有触发这个问题
```python3
    y_deep = tf.reshape(embeddings, shape=[-1, field_size * embedding_size])  # None * (F*K)
    y_deep = tf.nn.dropout(y_deep, dropout_keep_deep[0])
    for i in range(0, len(deep_layers)):
        # None * layer[i] * 1
        y_deep = tf.add(tf.matmul(y_deep, weights["layer_%d" % i]),weights["bias_%d" % i])
        y_deep = deep_layers_activation(y_deep)
        # dropout at each Deep layer
        y_deep = tf.nn.dropout(y_deep, dropout_keep_deep[1 + i]) 
```
多GPU时使用 ```optimizer.compute_gradients(loss)```会发现梯度里有操作变成了```None```如下图所示,在后续触发计算时就会报错了<br/>
![梯度里“操作”变成了None](https://raw.githubusercontent.com/Zachary4biz/Tahiti_any_data/master/pics/compute_gradients_error.png "Optional title")
    + 还是老老实实一层层实现layer更靠谱
```python3
    #### Deep-NN-model
    y_deep_input = tf.reshape(embeddings, shape=[-1, field_size * embedding_size])  # None * (F*K)
    y_deep_input = tf.nn.dropout(y_deep_input, dropout_keep_deep[0])
    # layer1
    y_deep_layer_0 = tf.add(tf.matmul(y_deep_input, weights["layer_0"]),weights["bias_0"])
    y_deep_layer_0 = deep_layers_activation(y_deep_layer_0)
    y_deep_layer_0 = tf.nn.dropout(y_deep_layer_0, dropout_keep_deep[1])
    # layer2
    y_deep_layer_1 = tf.add(tf.matmul(y_deep_layer_0, weights["layer_1"]),weights["bias_1"])
    y_deep_layer_1 = deep_layers_activation(y_deep_layer_1)
    y_deep_layer_1 = tf.nn.dropout(y_deep_layer_1, dropout_keep_deep[2])
    # layer3
    y_deep_layer_2 = tf.add(tf.matmul(y_deep_layer_1, weights["layer_2"]),weights["bias_2"])
    y_deep_layer_2 = deep_layers_activation(y_deep_layer_2)
    y_deep_layer_2 = tf.nn.dropout(y_deep_layer_2, dropout_keep_deep[2])
```



## 附
- #### <span id="DeepFM-graph">DeepFM模型计算图</span>
```python3
def deep_fm_graph(self,
                  weights,
                  feat_index,
                  feat_value,
                  train_phase):
    dropout_keep_fm = self.dropout_fm
    dropout_keep_deep = self.dropout_deep
    field_size = self.field_size
    embedding_size = self.embedding_size
    deep_layers = self.deep_layers
    deep_layers_activation = self.deep_layers_activation
    _batch_norm = self.batch_norm

    embeddings = tf.nn.embedding_lookup(weights["feature_embeddings"], feat_index)
    feat_value = tf.reshape(feat_value, shape=[-1, field_size, 1])
    embeddings = tf.multiply(embeddings, feat_value)
    # ---------- first order term ----------
    y_first_order = tf.nn.embedding_lookup(weights["feature_bias"], feat_index)  # None * F * 1
    y_first_order = tf.reduce_sum(tf.multiply(y_first_order, feat_value), 2)  # None * F
    y_first_order = tf.nn.dropout(y_first_order, dropout_keep_fm[0])  # None * F

    # ---------- second order term ---------------
    # sum_square part
    summed_features_emb = tf.reduce_sum(embeddings, 1)  # None * K
    summed_features_emb_square = tf.square(summed_features_emb)  # None * K

    # square_sum part
    squared_features_emb = tf.square(embeddings)
    squared_sum_features_emb = tf.reduce_sum(squared_features_emb, 1)  # None * K

    # second order
    y_second_order = 0.5 * tf.subtract(summed_features_emb_square,
                                            squared_sum_features_emb)  # None * K
    y_second_order = tf.nn.dropout(y_second_order, dropout_keep_fm[1])  # None * K

    # ---------- Deep component ----------
    # input
    y_deep_input = tf.reshape(embeddings, shape=[-1, field_size * embedding_size])  # None * (F*K)
    y_deep_input = tf.nn.dropout(y_deep_input, dropout_keep_deep[0])
    # layer1
    y_deep_layer_0 = tf.add(tf.matmul(y_deep_input, weights["layer_0"]),weights["bias_0"])
    y_deep_layer_0 = deep_layers_activation(y_deep_layer_0)
    y_deep_layer_0 = tf.nn.dropout(y_deep_layer_0, dropout_keep_deep[1])
    # layer2
    y_deep_layer_1 = tf.add(tf.matmul(y_deep_layer_0, weights["layer_1"]),weights["bias_1"])
    y_deep_layer_1 = deep_layers_activation(y_deep_layer_1)
    y_deep_layer_1 = tf.nn.dropout(y_deep_layer_1, dropout_keep_deep[2])
    # layer3
    y_deep_layer_2 = tf.add(tf.matmul(y_deep_layer_1, weights["layer_2"]),weights["bias_2"])
    y_deep_layer_2 = deep_layers_activation(y_deep_layer_2)
    y_deep_layer_2 = tf.nn.dropout(y_deep_layer_2, dropout_keep_deep[2])

    # ---------- DeepFM ----------
    concat_input = tf.concat([y_first_order, y_second_order, y_deep_layer_2], axis=1)
    out = tf.add(tf.matmul(concat_input, weights["concat_projection"]), weights["concat_bias"])
    return tf.nn.sigmoid(out)

```

- #### <span id="multi_gpu_graph">各个GPU在各自的命名空间创建计算图</span>
```python3
"""
deep_fm_graph() 函数为构建计算图
"""
models = []
for gpu_id in range(gpu_num):
    with tf.device("/gpu:%d" % gpu_id):
        with tf.name_scope("tower_%d" % gpu_id):
            with tf.variable_scope("cpu_varaibles", reuse=gpu_id>0):
                prefix = "tower_%d" % gpu_id
                f_idx = tf.placeholder(tf.int32, shape=[None, None], name=prefix+"_feat_index") # None * F
                f_v = tf.placeholder(tf.float32, shape=[None, None], name=prefix+"_feat_value") # None * F
                pred = deep_fm_graph(weights=weights,
                                     feat_index=f_idx,
                                     feat_value=f_v,
                                     train_phase=train_phase)
                label =  tf.placeholder(tf.float32, shape=[None, 1], name=prefix+"_label")  # None * 1
                loss = tf.reduce_mean(tf.losses.log_loss(label, pred))
                grads = _optimizer.compute_gradients(loss)
                models.append((f_idx,f_v,label,pred,loss,grads))
tower_f_idxs, tower_f_vs, tower_labels, tower_preds, tower_losses, tower_grads = zip(*models)
```

- #### <span id="average_gradient">average_gradient方法</span>
```python3
def average_gradients(tower_grads):
    from tensorflow.python.framework import ops
    print("average_gradients...")
    average_grads = []
    zip_target = list(zip(*tower_grads))
    for idx in range(len(zip_target)):
        grad_and_vars = zip_target[idx]
        grads = [g for g,_ in grad_and_vars]
        grad_stack = tf.stack(grads, 0)
        grad = tf.reduce_mean(grad_stack, 0)
        v = grad_and_vars[0][1]
        grad_and_var = (grad, v)
        average_grads.append(grad_and_var)
    print("运行过一次 average_gradients")
    return average_grads
```

- #### <span id="inside_struct_models">models内部变量详细结构</span>
    + feature_idx 样本特征索引
        * ```<tf.Tensor 'tower_0_4/cpu_varaibles/tower_0_feat_index:0' shape=(?, ?) dtype=int32>```
    + feature_v  样本特征的取值
        * ```<tf.Tensor 'tower_0_4/cpu_varaibles/tower_0_feat_value:0' shape=(?, ?) dtype=float32>```
    + label 样本标注
        * ```<tf.Tensor 'tower_0_4/cpu_varaibles/tower_0_label:0' shape=(?, 1) dtype=float32>```
    + pred  模型对样本的预计结果
        * ```<tf.Tensor 'tower_0_4/cpu_varaibles/Sigmoid:0' shape=(?, 1) dtype=float32>```
    + loss 损失
        * ```<tf.Tensor 'tower_0_4/cpu_varaibles/Mean:0' shape=() dtype=float32>```
    + grads: 
        * 结构类似 ```[(tensor, varaible),(tensor, varaible),(tensor, varaible)]```
        * grads[3]如下
            ```python3
            print(grads[3]) # (tensor, variable)
            # (<tf.Tensor 'tower_2_1/gpu_variables/gradients/tower_2_1/gpu_variables/MatMul_1_grad/tuple/control_dependency_1:0' shape=(32, 32) dtype=float32>, <tf.Variable 'w_layer_1:0' shape=(32, 32) dtype=float32_ref>)
            # tensor:
            <tf.Tensor 'tower_2_1/gpu_variables/gradients/tower_2_1/gpu_variables/MatMul_1_grad/tuple/control_dependency_1:0' shape=(32, 32) dtype=float32>
            # variable:
            <tf.Variable 'w_layer_1:0' shape=(32, 32) dtype=float32_ref>
            ```

- #### <span id="tower_grads">```tower_grads```触发计算后如下：</span>
```python3
"""
tower_grads_run[0][0] 是一个Tuple(tensor, variable)形式
"""
# tensor： tower_grads_run[0][0][0]
# 即结构中的 <tensorflow.python.framework.ops.IndexedSlices object at 0x7fd35777edd8>
IndexedSlicesValue(
    values=array([[6.11838323e-06, 2.62584490e-06, 8.88952684e-07, ...,
                   6.24857591e-07, 2.09694349e-06, -1.86265538e-06],
                  [-4.71157108e-07, -2.86395107e-07, -4.72639982e-07, ...,
                   2.62129078e-07, -1.14823976e-07, -9.66780220e-08],
                  [-5.51213475e-07, 4.50290509e-06, -1.35540938e-06, ...,
                   -2.36136043e-06, 3.75704417e-06, 8.80879907e-06],
                  ...,
                  [1.62912562e-04, 4.05646424e-05, -1.00826743e-04, ...,
                   2.31334652e-05, 4.97706569e-05, -2.01401999e-05],
                  [-6.17134501e-05, 7.85549128e-05, -2.05572607e-04, ...,
                   2.76901956e-05, 5.69441945e-05, 1.43811412e-04],
                  [4.59569783e-05, 1.23985257e-04, -2.93081230e-05, ...,
                   -4.00576846e-06, -5.10788232e-05, -1.17230287e-04]], dtype=float32),
    indices=array([0, 1, 2, ..., 100383, 109933, 110146], dtype=int32),
    dense_shape=array([117581, 8], dtype=int32)
)
# variable: tower_grads_run[0][0][1]
# 即结构中的 <tf.Variable 'feature_embeddings:0' shape=(117581, 8) dtype=float32_ref>
array([[6.59548864e-03, -8.61216243e-03, -1.10656880e-02, ...,
        5.53664193e-03, 4.57687909e-03, 6.36322703e-03],
       [1.25895496e-02, -5.49799390e-03, -8.62164609e-03, ...,
        3.61260050e-03, -9.74184461e-03, -1.98927079e-03],
       [1.57948416e-02, 7.99154118e-03, 7.93925574e-05, ...,
        1.62319895e-02, -1.92375924e-03, -1.28618791e-03],
       ...,
       [-2.95569026e-03, 2.36403360e-03, -5.19427191e-03, ...,
        -8.52187630e-03, -3.66753386e-03, -1.46189227e-03],
       [-4.85003879e-03, -1.09097324e-02, -4.51625884e-03, ...,
        1.02808578e-02, 1.16113005e-02, -6.02029171e-03],
       [8.50317520e-05, 1.33714220e-02, -1.21579589e-02, ...,
        -6.60297275e-03, -1.62493309e-03, 9.50708985e-03]], dtype=float32)
```

- #### <span id="criteo_data">Criteo数据</span>
    + [下载页面](http://labs.criteo.com/2014/02/download-kaggle-display-advertising-challenge-dataset/)
    + [kaggle页面](https://www.kaggle.com/c/criteo-display-ad-challenge)
    + 数据是13个连续特征，26个离散特征
    + 本示例使用的时候，对数据进行了处理：
        * 按```\t```分割成三块，依次为 ```连续特征的值```、```离散特征的索引(从0开始)```、```标注```
        ```python3
        0.05,0.006633,0.05,0,0.021594,0.008,0.15,0.04,0.362,0.1,0.2,0,0.04  2,542,1065,17784,26177,26328,28557,35348,35600,35971,48411,51351,64040,65951,66193,71615,84075,84106,86876,88267,88270,100275,100287,102434,109919,111810   0
        ```
        * 剔除了出现次数小于200的离散特征，各个特征填充各自的默认值
